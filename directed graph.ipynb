{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from Models import GateCNNFahsionSoftmax, CNNFashion, GateCNNFashion\n",
    "from Util import FedAvg, Preparedata, Sample_node\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Update import ClientUpdate\n",
    "\n",
    "import torch\n",
    "from torch import nn, autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fashionmnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset_train = datasets.FashionMNIST('../data/fashion-mnist', train=True, download = True, transform = trans_fashionmnist)\n",
    "dataset_test = datasets.FashionMNIST('../data/fashion-mnist',train=False, download = True, transform = trans_fashionmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 4\n",
    "ntrain = 1000\n",
    "ntest = 300\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "percent = 0.8\n",
    "#creat no-iid data\n",
    "# for i in range(num_clients):\n",
    "#     train_set, test_set = Preparedata(dataset_train, dataset_test, ntrain, ntest, class_number = [i%10], percent = percent)\n",
    "#     train_sets.append(train_set)\n",
    "#     test_sets.append(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [0,1,2], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)\n",
    "\n",
    "\n",
    "train_set, test_set = Preparedata(dataset_train, dataset_test, ntrain, ntest, class_number = [0,1,2], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)\n",
    "\n",
    "\n",
    "train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [5,6,8], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)\n",
    "\n",
    "\n",
    "train_set, test_set = Preparedata(dataset_train, dataset_test, ntrain, ntest, class_number = [5,6,8], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientUpdate(object):\n",
    "    def __init__(self, train_set=None,  test_set=None, idx=None):\n",
    "        \n",
    "        self.loss_func = nn.NLLLoss()\n",
    "        \n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(train_set, [800, 200], torch.Generator().manual_seed(idx))\n",
    "\n",
    "        self.ldr_train = DataLoader(self.train_set, batch_size=10, shuffle=True)\n",
    "        self.ldr_val = DataLoader(self.val_set, batch_size = 1, shuffle=True)\n",
    "\n",
    "        self.test_set = test_set\n",
    "        self.ldr_test = DataLoader(self.test_set, batch_size = 1, shuffle=True)\n",
    "    \n",
    "    def train(self, net, n_epochs,learning_rate):\n",
    "        \n",
    "        net.train()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "\n",
    "        epoch_loss = []\n",
    "\n",
    "        for iter in range(n_epochs):\n",
    "            net.train()\n",
    "            batch_loss = []\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images.float())\n",
    "    \n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_loss.append(loss.item())\n",
    "                \n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "#             val_acc, val_loss = self.validate(net,True)\n",
    "#             print(val_acc)\n",
    "\n",
    "        return net.state_dict(), epoch_loss[-1]\n",
    "   \n",
    "    \n",
    "    def train_finetune(self, net, n_epochs, learning_rate):\n",
    "        net.train()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "        \n",
    "        patience = 10\n",
    "        epoch_loss = []\n",
    "        epoch_train_accuracy = []\n",
    "        model_best = net.state_dict()\n",
    "        train_acc_best = np.inf\n",
    "        val_acc_best = -np.inf\n",
    "        val_loss_best = np.inf\n",
    "        counter = 0\n",
    "        \n",
    "        for iter in range(n_epochs):\n",
    "            net.train()\n",
    "            batch_loss = []\n",
    "            correct = 0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images.float())\n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "                _, predicted = torch.max(log_probs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            train_accuracy = 100.00 * correct / len(self.ldr_train.dataset)\n",
    "            epoch_train_accuracy.append(train_accuracy)\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "            if(iter%5==0):\n",
    "                val_acc, val_loss = self.validate(net,True)\n",
    "                net.train()\n",
    "                if(val_loss < val_loss_best - 0.01):\n",
    "                    counter = 0\n",
    "                    model_best = net.state_dict()\n",
    "                    val_acc_best = val_acc\n",
    "                    val_loss_best = val_loss\n",
    "                    train_acc_best = train_accuracy\n",
    "                    #print(\"Iter: %d | %.2f\" %(iter,val_acc_best))\n",
    "                else:\n",
    "                    counter = counter+1\n",
    "                    \n",
    "                # early stop\n",
    "                if counter == patience:\n",
    "                    return model_best, epoch_loss[-1], val_acc_best, train_acc_best\n",
    "                    \n",
    "    \n",
    "        return model_best, epoch_loss[-1], val_acc_best, train_acc_best\n",
    "     \n",
    "        \n",
    "    def train_mix(self, net_local, net_trans, gate, train_gate_only, n_epochs, early_stop, learning_rate, val):\n",
    "\n",
    "        print(\"start train mix model\")        \n",
    "        gate.train()\n",
    "        net_local.train()\n",
    "        net_trans.train()\n",
    "\n",
    "        if(train_gate_only):\n",
    "            optimizer = torch.optim.Adam(gate.parameters(),lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(list(gate.parameters())+list(net_local.parameters()),lr=learning_rate)\n",
    "\n",
    "      \n",
    "        patience = 30\n",
    "        epoch_loss = []\n",
    "        gate_best = gate.state_dict()\n",
    "        local_best = net_local.state_dict()\n",
    "        trans_best = net_trans.state_dict()\n",
    "        val_acc_best = -np.inf\n",
    "        val_loss_best = np.inf\n",
    "        counter = 0\n",
    "        gate_values_best = 0\n",
    "\n",
    "        \n",
    "        for iter in range(n_epochs):\n",
    "            net_local.train()\n",
    "            net_trans.train()\n",
    "            gate.train()\n",
    "\n",
    "            batch_loss = []\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "\n",
    "                net_local.zero_grad()\n",
    "                net_trans.zero_grad()\n",
    "                gate.zero_grad()\n",
    "\n",
    "                gate_weight = gate(images.float())\n",
    "            \n",
    "\n",
    "                # gate_weight*wi + gate_weight*fintuned\n",
    "                local_prob = gate_weight*net_local(images.float())+(1-gate_weight)*net_trans(images.float())\n",
    "                loss = self.loss_func(local_prob,labels)\n",
    "  \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "            \n",
    "            if(early_stop):\n",
    "                if(iter%5==0):\n",
    "                    val_acc, val_loss = self.validate_mix(net_local, net_trans, gate, val)\n",
    "                    net_local.train()\n",
    "                    net_trans.train()\n",
    "                    gate.train()\n",
    "\n",
    "                    if(val_loss < val_loss_best - 0.01):\n",
    "                        counter = 0\n",
    "                        gate_best = gate.state_dict()\n",
    "                        local_best = net_local.state_dict()\n",
    "                        trans_best = net_trans.state_dict()\n",
    "                        val_acc_best = val_acc\n",
    "                        val_loss_best = val_loss\n",
    "                        gate_weight_best = gate_weight\n",
    "                        print(\"Iter: %d | %.2f\" %(iter,val_acc_best))\n",
    "                    else:\n",
    "                        counter = counter + 1\n",
    "                        #print(counter)\n",
    "                \n",
    "                if counter == patience:\n",
    "                    return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, sum(gate_weight_best) / len(gate_weight_best)\n",
    "\n",
    "\n",
    "        return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, sum(gate_weight_best) / len(gate_weight_best)\n",
    "    \n",
    "        \n",
    "    def train_mix_clients(self, net_local, net_trans, net_clients, gate_frezze, gate, train_gate_only, n_epochs, early_stop, learning_rate, val):\n",
    "\n",
    "        print(\"start train mix model\")        \n",
    "        gate.train()\n",
    "        net_local.train()\n",
    "        net_trans.train()\n",
    "\n",
    "        if(train_gate_only):\n",
    "            optimizer = torch.optim.Adam(gate.parameters(),lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(list(gate.parameters())+list(net_local.parameters()),lr=learning_rate)\n",
    "\n",
    "      \n",
    "        patience = 30\n",
    "        epoch_loss = []\n",
    "        gate_best = gate.state_dict()\n",
    "        local_best = net_local.state_dict()\n",
    "        trans_best = net_trans.state_dict()\n",
    "        val_acc_best = -np.inf\n",
    "        val_loss_best = np.inf\n",
    "        counter = 0\n",
    "        gate_values_best = 0\n",
    "\n",
    "        \n",
    "        for iter in range(n_epochs):\n",
    "            net_local.train()\n",
    "            net_trans.train()\n",
    "            gate.train()\n",
    "\n",
    "            batch_loss = []\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "\n",
    "                net_local.zero_grad()\n",
    "                net_trans.zero_grad()\n",
    "                gate.zero_grad()\n",
    "\n",
    "                gate_inner = gate_frezze(images.float())\n",
    "                gate_outer = gate(images.float())\n",
    "             \n",
    "\n",
    "                # gate_weight*wi + gate_weight*fintuned\n",
    "                local_prob_inner = gate_inner*net_local(images.float())+(1-gate_inner)*net_trans(images.float())\n",
    "                local_prob = gate_outer*net_clients(images.float())+(1-gate_outer)*local_prob_inner\n",
    "                loss = self.loss_func(local_prob,labels)\n",
    "  \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "            \n",
    "            if(early_stop):\n",
    "                if(iter%5==0):\n",
    "                    val_acc, val_loss = self.validate_mix_clients(net_local, net_trans, net_clients, gate_frezze, gate, val)\n",
    "                    net_local.train()\n",
    "                    net_trans.train()\n",
    "                    gate.train()\n",
    "\n",
    "                    if(val_loss < val_loss_best - 0.01):\n",
    "                        counter = 0\n",
    "                        gate_best = gate.state_dict()\n",
    "                        local_best = net_local.state_dict()\n",
    "                        trans_best = net_trans.state_dict()\n",
    "                        val_acc_best = val_acc\n",
    "                        val_loss_best = val_loss\n",
    "                        gate_weight_best = weight\n",
    "                        print(\"Iter: %d | %.2f\" %(iter,val_acc_best))\n",
    "                    else:\n",
    "                        counter = counter + 1\n",
    "                        #print(counter)\n",
    "                \n",
    "                if counter == patience:\n",
    "                    return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, gate_weight_best\n",
    "\n",
    "\n",
    "        return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, gate_outer\n",
    "    \n",
    "    \n",
    "    def validate(self,net,val):\n",
    "        # if true validate dataset, if false use test detaset\n",
    "        if(val):\n",
    "            dataloader = self.ldr_val\n",
    "        else:\n",
    "            dataloader = self.ldr_test\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            # validate\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            for idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = (data, target)\n",
    "                log_probs = net(data.float())\n",
    "\n",
    "                val_loss += self.loss_func(log_probs, target).item()\n",
    "\n",
    "                y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "                correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "            val_loss /= len(dataloader.dataset)\n",
    "            accuracy = 100.00 * correct / len(dataloader.dataset)\n",
    "   \n",
    "        return accuracy.item(), val_loss\n",
    "    \n",
    "    def validate_mix(self, net_l, net_t, gate, val):\n",
    "        # if true validate dataset, if false use test detaset\n",
    "        if(val):\n",
    "            dataloader = self.ldr_val\n",
    "        else:\n",
    "            dataloader = self.ldr_test\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net_l.eval()\n",
    "            net_t.eval()\n",
    "            gate.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            gate_values = np.array([])\n",
    "            label_values = np.array([])\n",
    "            \n",
    "            for idx, (data,target) in enumerate(dataloader):\n",
    "                data, target = (data,target)\n",
    "                gate_weight = gate(data.float())\n",
    "                \n",
    "                log_prob = gate_weight*net_l(data.float())+(1-gate_weight)*net_t(data.float())\n",
    "                \n",
    "\n",
    "                val_loss += self.loss_func(log_prob,target).item()\n",
    "                y_pred = log_prob.data.max(1,keepdim=True)[1]\n",
    "                correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "        val_loss /= len(dataloader.dataset)\n",
    "        accuracy = 100.00 * correct / len(dataloader.dataset)\n",
    "        return accuracy.item(), val_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def validate_mix_clients(self, net_l, net_t, net_c, gate_f, gate, val):\n",
    "        # if true validate dataset, if false use test detaset\n",
    "        if(val):\n",
    "            dataloader = self.ldr_val\n",
    "        else:\n",
    "            dataloader = self.ldr_test\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net_l.eval()\n",
    "            net_t.eval()\n",
    "            gate.eval()\n",
    "            net_c.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            gate_values = np.array([])\n",
    "            label_values = np.array([])\n",
    "            \n",
    "            for idx, (data,target) in enumerate(dataloader):\n",
    "                data, target = (data,target)\n",
    "                gate_inner = gate_f(data.float())\n",
    "                gate_outer = gate(data.float())\n",
    "                \n",
    "                local_prob_inner = gate_inner*net_l(data.float())+(1-gate_inner)*net_t(data.float())\n",
    "                log_prob = gate_outer*net_c(data.float())+(1-gate_outer)*local_prob_inner\n",
    "\n",
    "                val_loss += self.loss_func(log_prob,target).item()\n",
    "                y_pred = log_prob.data.max(1,keepdim=True)[1]\n",
    "                correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "        val_loss /= len(dataloader.dataset)\n",
    "        accuracy = 100.00 * correct / len(dataloader.dataset)\n",
    "        return accuracy.item(), val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_locals = [] # save the local model of every client\n",
    "\n",
    "for idx in range(num_clients):\n",
    "    net_local = CNNFashion(num_classes)\n",
    "    net_locals.append(net_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ep = 100\n",
    "lr = 1e-4\n",
    "\n",
    "w_fedAvg = []\n",
    "alpha = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "w_clients = []\n",
    "net_fed = []\n",
    "val_local = []\n",
    "fintuned_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "net_glob_fedAvg = CNNFashion(num_classes)\n",
    "\n",
    "net_locals = []\n",
    "for idx in range(num_clients):\n",
    "    net_local = CNNFashion(num_classes)\n",
    "    net_locals.append(net_local)\n",
    "    \n",
    "acc = []\n",
    "w_fedAvg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.ones( (num_clients, num_clients) )*(1/(num_clients-1))\n",
    "\n",
    "for idx in range(num_clients):\n",
    "    G[idx][idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = np.zeros( (num_clients, num_clients) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0\n",
      "Client 1\n",
      "Client 2\n",
      "Client 3\n",
      "[78.0, 79.0, 76.0, 76.66666412353516]\n",
      "Client 0 and  1\n",
      "Client 0\n",
      "Client 1\n",
      "[77.66666412353516, 83.66666412353516]\n",
      "Client 0 and  2\n",
      "Client 0\n",
      "Client 2\n",
      "[83.66666412353516, 80.66666412353516]\n",
      "Client 0 and  3\n",
      "Client 0\n",
      "Client 3\n",
      "[83.66666412353516, 80.33333587646484]\n",
      "Client 1 and  2\n",
      "Client 1\n",
      "Client 2\n",
      "[87.0, 62.66666793823242]\n",
      "Client 1 and  3\n",
      "Client 1\n",
      "Client 3\n",
      "[83.33333587646484, 61.0]\n",
      "Client 2 and  3\n",
      "Client 2\n",
      "Client 3\n",
      "[76.66666412353516, 77.66666412353516]\n",
      "[78.0, 79.0, 76.0, 76.66666412353516]\n",
      "Client 0 and  1\n",
      "Client 0\n",
      "Client 1\n",
      "[78.0, 83.66666412353516]\n",
      "Client 0 and  2\n",
      "Client 0\n",
      "Client 2\n",
      "[83.66666412353516, 80.66666412353516]\n",
      "Client 0 and  3\n",
      "Client 0\n",
      "Client 3\n",
      "[84.0, 80.0]\n",
      "Client 1 and  2\n",
      "Client 1\n",
      "Client 2\n",
      "[86.33333587646484, 63.33333206176758]\n",
      "Client 1 and  3\n",
      "Client 1\n",
      "Client 3\n",
      "[84.66666412353516, 62.33333206176758]\n",
      "Client 2 and  3\n",
      "Client 2\n",
      "Client 3\n",
      "[76.66666412353516, 77.66666412353516]\n",
      "[78.0, 79.0, 76.0, 76.66666412353516]\n",
      "Client 0 and  1\n",
      "Client 0\n",
      "Client 1\n",
      "[78.0, 83.33333587646484]\n",
      "Client 0 and  2\n",
      "Client 0\n",
      "Client 2\n",
      "[84.0, 80.0]\n",
      "Client 0 and  3\n",
      "Client 0\n",
      "Client 3\n",
      "[84.0, 79.66666412353516]\n",
      "Client 1 and  2\n",
      "Client 1\n",
      "Client 2\n",
      "[86.66666412353516, 62.66666793823242]\n",
      "Client 1 and  3\n",
      "Client 1\n",
      "Client 3\n",
      "[29.66666603088379, 76.0]\n",
      "Client 2 and  3\n",
      "Client 2\n",
      "Client 3\n",
      "[77.33333587646484, 77.66666412353516]\n",
      "[78.0, 79.0, 76.0, 76.66666412353516]\n",
      "Client 0 and  1\n",
      "Client 0\n",
      "Client 1\n",
      "[84.33333587646484, 84.33333587646484]\n",
      "Client 0 and  2\n",
      "Client 0\n",
      "Client 2\n",
      "[84.0, 80.0]\n",
      "Client 0 and  3\n",
      "Client 0\n",
      "Client 3\n",
      "[83.33333587646484, 79.33333587646484]\n",
      "Client 1 and  2\n",
      "Client 1\n",
      "Client 2\n",
      "[87.0, 82.0]\n",
      "Client 1 and  3\n",
      "Client 1\n",
      "Client 3\n",
      "[86.66666412353516, 62.33333206176758]\n",
      "Client 2 and  3\n",
      "Client 2\n",
      "Client 3\n",
      "[77.0, 78.0]\n",
      "[78.0, 79.0, 76.0, 76.66666412353516]\n",
      "Client 0 and  1\n",
      "Client 0\n",
      "Client 1\n",
      "[78.0, 83.66666412353516]\n",
      "Client 0 and  2\n",
      "Client 0\n",
      "Client 2\n",
      "[83.33333587646484, 80.66666412353516]\n",
      "Client 0 and  3\n",
      "Client 0\n",
      "Client 3\n",
      "[83.33333587646484, 79.66666412353516]\n",
      "Client 1 and  2\n",
      "Client 1\n",
      "Client 2\n",
      "[86.66666412353516, 83.33333587646484]\n",
      "Client 1 and  3\n",
      "Client 1\n",
      "Client 3\n",
      "[86.66666412353516, 63.0]\n",
      "Client 2 and  3\n",
      "Client 2\n",
      "Client 3\n",
      "[77.0, 77.0]\n",
      "[78.0, 79.0, 76.0, 76.66666412353516]\n"
     ]
    }
   ],
   "source": [
    "# for loop in range(5):\n",
    "    # tain every client seperately\n",
    "#     train_set = []\n",
    "#     test_set = []\n",
    "#     train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [0,1,2], percent = percent)\n",
    "#     train_sets.append(train_set)\n",
    "#     test_sets.append(test_set)\n",
    "\n",
    "\n",
    "#     train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [0,1,2], percent = percent)\n",
    "#     train_sets.append(train_set)\n",
    "#     test_sets.append(test_set)\n",
    "\n",
    "\n",
    "#     train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [5,6,8], percent = percent)\n",
    "#     train_sets.append(train_set)\n",
    "#     test_sets.append(test_set)\n",
    "\n",
    "\n",
    "#     train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [5,6,8], percent = percent)\n",
    "#     train_sets.append(train_set)\n",
    "#     test_sets.append(test_set)\n",
    "val_local = []\n",
    "for idx in range(num_clients):\n",
    "        print(\"Client %d\" %(idx))\n",
    "\n",
    "\n",
    "        client = ClientUpdate(train_set=train_sets[idx], test_set = test_sets[idx],\n",
    "                                   idx = idx)\n",
    "\n",
    "        # trian the ith w\n",
    "        net_updated, train_loss_idx = client.train(net = net_locals[idx], n_epochs = 150,learning_rate = 1e-4)\n",
    "\n",
    "        w_fedAvg.append(copy.deepcopy(net_updated))\n",
    "\n",
    "        net_locals[idx].load_state_dict(w_fedAvg[idx])\n",
    "\n",
    "        train_loss.append(train_loss_idx)\n",
    "\n",
    "        # envaluate the local nets\n",
    "        local_val_acc, val_loss = client.validate(net_locals[idx],False)\n",
    "        val_local.append(local_val_acc)\n",
    "\n",
    "\n",
    "print(val_local)\n",
    "   \n",
    "for loop in range(5):\n",
    "    for idx in range(num_clients):\n",
    "        for i in range(num_clients):\n",
    "            if (i > idx):\n",
    "                print(\"Client %d and % d\" %(idx, i))\n",
    "                w = [w_fedAvg[idx],w_fedAvg[i]]\n",
    "                alpha = [0.5,0.5]\n",
    "                w_global = FedAvg(w,alpha)\n",
    "                net_glob_fedAvg.load_state_dict(w_global)\n",
    "\n",
    "\n",
    "                fintuned_nets = []\n",
    "                fintuned_acc = []\n",
    "                for j in [idx,i]:\n",
    "                        print(\"Client %d\" %(j))\n",
    "\n",
    "\n",
    "                        client = ClientUpdate(train_set=train_sets[j], test_set = test_sets[j],\n",
    "                                                   idx = j)\n",
    "\n",
    "                        # trian the ith w\n",
    "                        net_w, train_loss_idx, val_acc_idx, train_acc_idx = client.train_finetune(net = net_glob_fedAvg, \n",
    "                                                                n_epochs = 150, learning_rate = 1e-4)\n",
    "\n",
    "                        ft_net = copy.deepcopy(net_glob_fedAvg)\n",
    "                        ft_net.load_state_dict(net_w)\n",
    "                        fintuned_nets.append(ft_net)\n",
    "\n",
    "\n",
    "                        fintuned_val_acc, val_loss = client.validate(ft_net,False)\n",
    "                        fintuned_acc.append(fintuned_val_acc)\n",
    "\n",
    "                print(fintuned_acc)\n",
    "\n",
    "                if(fintuned_acc[0] > val_local[idx]+1):\n",
    "                    graph[idx,i]=graph[idx,i]+1\n",
    "                elif(fintuned_acc[0] < val_local[idx]-1):\n",
    "                    graph[idx,i]=graph[idx,i]-1\n",
    "\n",
    "                if(fintuned_acc[1] > val_local[i]+1):\n",
    "                    graph[i,idx]=graph[i,idx]+1\n",
    "                elif(fintuned_acc[1] < val_local[i]-1):\n",
    "                    graph[i,idx]=graph[i,idx]-1\n",
    "\n",
    "\n",
    "    print(val_local)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  5.,  5.],\n",
       "       [ 5.,  0.,  5.,  3.],\n",
       "       [ 5., -1.,  0.,  1.],\n",
       "       [ 5., -4.,  1.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weight according the graph\n",
    "# the sum of every row quales one\n",
    "for i in range(num_clients):\n",
    "    for j in range(num_clients):\n",
    "        if (graph[i][j]==5):\n",
    "            G[i][j] = G[i][j]*1.3\n",
    "        elif (graph[i][j]==4):\n",
    "            G[i][j] = G[i][j]*1.2\n",
    "        elif (graph[i][j]==3):\n",
    "            G[i][j] = G[i][j]*1.1\n",
    "        elif (graph[i][j]== -5):\n",
    "            G[i][j] = G[i][j]/1.3\n",
    "        elif (graph[i][j]== -4):\n",
    "            G[i][j] = G[i][j]/1.2\n",
    "        elif (graph[i][j]== -3):\n",
    "            G[i][j] = G[i][j]/1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(num_clients):\n",
    "    G[idx] = [x / sum(G[idx]) for x in G[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.27777778, 0.36111111, 0.36111111],\n",
       "       [0.35135135, 0.        , 0.35135135, 0.2972973 ],\n",
       "       [0.39393939, 0.3030303 , 0.        , 0.3030303 ],\n",
       "       [0.41489362, 0.26595745, 0.31914894, 0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
