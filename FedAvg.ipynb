{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code changed from graph papaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from Models import GateCNNFahsionSoftmax, CNNFashion, GateCNNFashion\n",
    "from Util import FedAvg, Preparedata, Sample_node\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Update import ClientUpdate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fashionmnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset_train = datasets.FashionMNIST('../data/fashion-mnist', train=True, download = True, transform = trans_fashionmnist)\n",
    "dataset_test = datasets.FashionMNIST('../data/fashion-mnist',train=False, download = True, transform = trans_fashionmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 4\n",
    "ntrain = 1000\n",
    "ntest = 300\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "percent = 0.8\n",
    "\n",
    "train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [0,1,2], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)\n",
    "\n",
    "train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [1,2,4], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)\n",
    "\n",
    "\n",
    "train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [6,8,9], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)\n",
    "\n",
    "train_set, test_set = Preparedata(shuffle(dataset_train), dataset_test, ntrain, ntest, class_number = [5,6,8], percent = percent)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, autograd\n",
    "\n",
    "class ClientUpdate(object):\n",
    "    def __init__(self, train_set=None,  test_set=None, idx=None):\n",
    "        \n",
    "        self.loss_func = nn.NLLLoss()\n",
    "        \n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(train_set, [800, 200], torch.Generator().manual_seed(idx))\n",
    "\n",
    "        self.ldr_train = DataLoader(self.train_set, batch_size=10, shuffle=True)\n",
    "        self.ldr_val = DataLoader(self.val_set, batch_size = 1, shuffle=True)\n",
    "\n",
    "        self.test_set = test_set\n",
    "        self.ldr_test = DataLoader(self.test_set, batch_size = 1, shuffle=True)\n",
    "    \n",
    "    def train(self, net, n_epochs,learning_rate):\n",
    "        \n",
    "        net.train()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "\n",
    "        epoch_loss = []\n",
    "\n",
    "        for iter in range(n_epochs):\n",
    "            net.train()\n",
    "            batch_loss = []\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images.float())\n",
    "    \n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_loss.append(loss.item())\n",
    "                \n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "#             val_acc, val_loss = self.validate(net,True)\n",
    "#             print(val_acc)\n",
    "\n",
    "        return net.state_dict(), epoch_loss[-1]\n",
    "   \n",
    "    \n",
    "    def train_finetune(self, net, n_epochs, learning_rate):\n",
    "        net.train()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "        \n",
    "        patience = 10\n",
    "        epoch_loss = []\n",
    "        epoch_train_accuracy = []\n",
    "        model_best = net.state_dict()\n",
    "        train_acc_best = np.inf\n",
    "        val_acc_best = -np.inf\n",
    "        val_loss_best = np.inf\n",
    "        counter = 0\n",
    "        \n",
    "        for iter in range(n_epochs):\n",
    "            net.train()\n",
    "            batch_loss = []\n",
    "            correct = 0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images.float())\n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "                _, predicted = torch.max(log_probs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            train_accuracy = 100.00 * correct / len(self.ldr_train.dataset)\n",
    "            epoch_train_accuracy.append(train_accuracy)\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "            if(iter%5==0):\n",
    "                val_acc, val_loss = self.validate(net,True)\n",
    "                net.train()\n",
    "                if(val_loss < val_loss_best - 0.01):\n",
    "                    counter = 0\n",
    "                    model_best = net.state_dict()\n",
    "                    val_acc_best = val_acc\n",
    "                    val_loss_best = val_loss\n",
    "                    train_acc_best = train_accuracy\n",
    "                    print(\"Iter: %d | %.2f\" %(iter,val_acc_best))\n",
    "                else:\n",
    "                    counter = counter+1\n",
    "                    \n",
    "                # early stop\n",
    "                if counter == patience:\n",
    "                    return model_best, epoch_loss[-1], val_acc_best, train_acc_best\n",
    "                    \n",
    "    \n",
    "        return model_best, epoch_loss[-1], val_acc_best, train_acc_best\n",
    "     \n",
    "        \n",
    "    def train_mix(self, net_local, net_trans, gate, train_gate_only, n_epochs, early_stop, learning_rate, val):\n",
    "\n",
    "        print(\"start train mix model\")        \n",
    "        gate.train()\n",
    "        net_local.train()\n",
    "        net_trans.train()\n",
    "\n",
    "        if(train_gate_only):\n",
    "            optimizer = torch.optim.Adam(gate.parameters(),lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(list(gate.parameters())+list(net_local.parameters()),lr=learning_rate)\n",
    "\n",
    "      \n",
    "        patience = 30\n",
    "        epoch_loss = []\n",
    "        gate_best = gate.state_dict()\n",
    "        local_best = net_local.state_dict()\n",
    "        trans_best = net_trans.state_dict()\n",
    "        val_acc_best = -np.inf\n",
    "        val_loss_best = np.inf\n",
    "        counter = 0\n",
    "        gate_values_best = 0\n",
    "\n",
    "        \n",
    "        for iter in range(n_epochs):\n",
    "            net_local.train()\n",
    "            net_trans.train()\n",
    "            gate.train()\n",
    "\n",
    "            batch_loss = []\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "\n",
    "                net_local.zero_grad()\n",
    "                net_trans.zero_grad()\n",
    "                gate.zero_grad()\n",
    "\n",
    "                gate_weight = gate(images.float())\n",
    "            \n",
    "\n",
    "                # gate_weight*wi + gate_weight*fintuned\n",
    "                local_prob = gate_weight*net_local(images.float())+(1-gate_weight)*net_trans(images.float())\n",
    "                loss = self.loss_func(local_prob,labels)\n",
    "  \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "            \n",
    "            if(early_stop):\n",
    "                if(iter%5==0):\n",
    "                    val_acc, val_loss = self.validate_mix(net_local, net_trans, gate, val)\n",
    "                    net_local.train()\n",
    "                    net_trans.train()\n",
    "                    gate.train()\n",
    "\n",
    "                    if(val_loss < val_loss_best - 0.01):\n",
    "                        counter = 0\n",
    "                        gate_best = gate.state_dict()\n",
    "                        local_best = net_local.state_dict()\n",
    "                        trans_best = net_trans.state_dict()\n",
    "                        val_acc_best = val_acc\n",
    "                        val_loss_best = val_loss\n",
    "                        gate_weight_best = gate_weight\n",
    "                        print(\"Iter: %d | %.2f\" %(iter,val_acc_best))\n",
    "                    else:\n",
    "                        counter = counter + 1\n",
    "                        #print(counter)\n",
    "                \n",
    "                if counter == patience:\n",
    "                    return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, sum(gate_weight_best) / len(gate_weight_best)\n",
    "\n",
    "\n",
    "        return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, sum(gate_weight_best) / len(gate_weight_best)\n",
    "    \n",
    "        \n",
    "    def train_mix_clients(self, net_local, net_trans, net_clients, gate_frezze, gate, train_gate_only, n_epochs, early_stop, learning_rate, val):\n",
    "\n",
    "        print(\"start train mix model\")        \n",
    "        gate.train()\n",
    "        net_local.train()\n",
    "        net_trans.train()\n",
    "\n",
    "        if(train_gate_only):\n",
    "            optimizer = torch.optim.Adam(gate.parameters(),lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(list(gate.parameters())+list(net_local.parameters()),lr=learning_rate)\n",
    "\n",
    "      \n",
    "        patience = 30\n",
    "        epoch_loss = []\n",
    "        gate_best = gate.state_dict()\n",
    "        local_best = net_local.state_dict()\n",
    "        trans_best = net_trans.state_dict()\n",
    "        val_acc_best = -np.inf\n",
    "        val_loss_best = np.inf\n",
    "        counter = 0\n",
    "        gate_values_best = 0\n",
    "\n",
    "        \n",
    "        for iter in range(n_epochs):\n",
    "            net_local.train()\n",
    "            net_trans.train()\n",
    "            gate.train()\n",
    "\n",
    "            batch_loss = []\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = (images, labels)\n",
    "\n",
    "                net_local.zero_grad()\n",
    "                net_trans.zero_grad()\n",
    "                gate.zero_grad()\n",
    "\n",
    "                gate_inner = gate_frezze(images.float())\n",
    "                gate_outer = gate(images.float())\n",
    "             \n",
    "\n",
    "                # gate_weight*wi + gate_weight*fintuned\n",
    "                local_prob_inner = gate_inner*net_local(images.float())+(1-gate_inner)*net_trans(images.float())\n",
    "                local_prob = gate_outer*net_clients(images.float())+(1-gate_outer)*local_prob_inner\n",
    "                loss = self.loss_func(local_prob,labels)\n",
    "  \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "            \n",
    "            \n",
    "            if(early_stop):\n",
    "                if(iter%5==0):\n",
    "                    val_acc, val_loss = self.validate_mix_clients(net_local, net_trans, net_clients, gate_frezze, gate, val)\n",
    "                    net_local.train()\n",
    "                    net_trans.train()\n",
    "                    gate.train()\n",
    "\n",
    "                    if(val_loss < val_loss_best - 0.01):\n",
    "                        counter = 0\n",
    "                        gate_best = gate.state_dict()\n",
    "                        local_best = net_local.state_dict()\n",
    "                        trans_best = net_trans.state_dict()\n",
    "                        val_acc_best = val_acc\n",
    "                        val_loss_best = val_loss\n",
    "                        gate_weight_best = weight\n",
    "                        print(\"Iter: %d | %.2f\" %(iter,val_acc_best))\n",
    "                    else:\n",
    "                        counter = counter + 1\n",
    "                        #print(counter)\n",
    "                \n",
    "                if counter == patience:\n",
    "                    return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, gate_weight_best\n",
    "\n",
    "\n",
    "        return gate_best, local_best, trans_best, epoch_loss[-1],val_acc_best, gate_outer\n",
    "    \n",
    "    \n",
    "    def validate(self,net,val):\n",
    "        # if true validate dataset, if false use test detaset\n",
    "        if(val):\n",
    "            dataloader = self.ldr_val\n",
    "        else:\n",
    "            dataloader = self.ldr_test\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            # validate\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            for idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = (data, target)\n",
    "                log_probs = net(data.float())\n",
    "\n",
    "                val_loss += self.loss_func(log_probs, target).item()\n",
    "\n",
    "                y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "                correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "            val_loss /= len(dataloader.dataset)\n",
    "            accuracy = 100.00 * correct / len(dataloader.dataset)\n",
    "   \n",
    "        return accuracy.item(), val_loss\n",
    "    \n",
    "    def validate_mix(self, net_l, net_t, gate, val):\n",
    "        # if true validate dataset, if false use test detaset\n",
    "        if(val):\n",
    "            dataloader = self.ldr_val\n",
    "        else:\n",
    "            dataloader = self.ldr_test\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net_l.eval()\n",
    "            net_t.eval()\n",
    "            gate.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            gate_values = np.array([])\n",
    "            label_values = np.array([])\n",
    "            \n",
    "            for idx, (data,target) in enumerate(dataloader):\n",
    "                data, target = (data,target)\n",
    "                gate_weight = gate(data.float())\n",
    "                \n",
    "                log_prob = gate_weight*net_l(data.float())+(1-gate_weight)*net_t(data.float())\n",
    "                \n",
    "\n",
    "                val_loss += self.loss_func(log_prob,target).item()\n",
    "                y_pred = log_prob.data.max(1,keepdim=True)[1]\n",
    "                correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "        val_loss /= len(dataloader.dataset)\n",
    "        accuracy = 100.00 * correct / len(dataloader.dataset)\n",
    "        return accuracy.item(), val_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def validate_mix_clients(self, net_l, net_t, net_c, gate_f, gate, val):\n",
    "        # if true validate dataset, if false use test detaset\n",
    "        if(val):\n",
    "            dataloader = self.ldr_val\n",
    "        else:\n",
    "            dataloader = self.ldr_test\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net_l.eval()\n",
    "            net_t.eval()\n",
    "            gate.eval()\n",
    "            net_c.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            gate_values = np.array([])\n",
    "            label_values = np.array([])\n",
    "            \n",
    "            for idx, (data,target) in enumerate(dataloader):\n",
    "                data, target = (data,target)\n",
    "                gate_inner = gate_f(data.float())\n",
    "                gate_outer = gate(data.float())\n",
    "                \n",
    "                local_prob_inner = gate_inner*net_l(data.float())+(1-gate_inner)*net_t(data.float())\n",
    "                log_prob = gate_outer*net_c(data.float())+(1-gate_outer)*local_prob_inner\n",
    "\n",
    "                val_loss += self.loss_func(log_prob,target).item()\n",
    "                y_pred = log_prob.data.max(1,keepdim=True)[1]\n",
    "                correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "        val_loss /= len(dataloader.dataset)\n",
    "        accuracy = 100.00 * correct / len(dataloader.dataset)\n",
    "        return accuracy.item(), val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_glob_fedAvg = CNNFashion(num_classes)\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "net_locals = []\n",
    "for idx in range(num_clients):\n",
    "    net_local = CNNFashion(num_classes)\n",
    "    net_locals.append(net_local)\n",
    "    \n",
    "acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0\n",
      "Client 1\n",
      "Client 2\n",
      "Client 3\n",
      "86.83333206176758\n",
      "Client 0\n",
      "Client 1\n",
      "Client 2\n",
      "Client 3\n",
      "86.16666603088379\n"
     ]
    }
   ],
   "source": [
    "for round in range(10):\n",
    "    w_fedAvg = []\n",
    "    train_loss = []\n",
    "    val_local = []\n",
    "    \n",
    "    for idx in range(num_clients):\n",
    "        print(\"Client %d\" %(idx))\n",
    "        client = ClientUpdate(train_set=train_sets[idx], test_set = test_sets[idx],\n",
    "                               idx = idx)\n",
    "\n",
    "        # trian the ith w\n",
    "        net_updated, train_loss_idx = client.train(net = net_glob_fedAvg, n_epochs = 100,learning_rate = 1e-4)\n",
    "\n",
    "        w_fedAvg.append(copy.deepcopy(net_updated))\n",
    "\n",
    "        net_locals[idx].load_state_dict(w_fedAvg[idx])\n",
    "\n",
    "        train_loss.append(train_loss_idx)\n",
    "\n",
    "        # envaluate the local nets\n",
    "        local_val_acc, val_loss = client.validate(net_locals[idx],False)\n",
    "        val_local.append(local_val_acc)\n",
    "    \n",
    "    acc_iter = np.average(val_local)\n",
    "    print(acc_iter)\n",
    "    acc.append(acc_iter)\n",
    "    \n",
    "    # get the fedavg without weight\n",
    "    \n",
    "    net_glob_fedAvg = CNNFashion(num_classes)\n",
    "\n",
    "    alpha = [1/num_clients]*num_clients\n",
    "\n",
    "    w_global = FedAvg(w_fedAvg,alpha)\n",
    "    net_glob_fedAvg.load_state_dict(w_global)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda931e90a0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
